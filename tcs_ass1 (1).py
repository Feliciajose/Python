# -*- coding: utf-8 -*-
"""tcs ass1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dmQdyw5CcUHg4l8iUJDYqSvWugn89P7E
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('gdpWorld.csv')
print(df)

print(df.columns)

import pandas as pd

# Read the CSV file
df = pd.read_csv('gdpWorld.csv')

# Check the completeness of data collection
print(df.isnull().sum())

# Check the correctness of data collection
print(df.describe())

import pandas as pd
from scipy import stats

# List of columns that may contain comma values
columns_to_convert = ['Population', 'Area (sq. mi.)', 'Coastline (coast/area ratio)', 'Net migration',
                       'Infant mortality (per 1000 births)', 'GDP ($ per capita)', 'Literacy (%)',
                       'Phones (per 1000)', 'Arable (%)', 'Crops (%)', 'Birthrate', 'Deathrate',
                       'Agriculture', 'Industry', 'Service', 'Other (%)', 'Pop. Density (per sq. mi.)']

# Convert commas to dots in numeric columns
for column in columns_to_convert:
    df[column] = df[column].astype(str).str.replace(',', '.').astype(float)
for column in columns_to_convert:
    df[column] = pd.to_numeric(df[column].replace('[^\d.]', '', regex=True), errors='coerce')

# Drop rows with NaN values
# Remove NaN and null values
df_cleaned = df.dropna()

# Remove outliers using Z-score
z_threshold = 3
z_scores = stats.zscore(df_cleaned.select_dtypes(include=['number']))
outliers = (abs(z_scores) > z_threshold).any(axis=1)
df_no_outliers = df_cleaned[~outliers]

# Display the modified dataframe
print(df_no_outliers.head())

# Selecting relevant features
features = ['Population','Area (sq. mi.)','Phones (per 1000)', 'Literacy (%)','Birthrate','Deathrate']
print(features)

# Extracting features and target variable
X = df_no_outliers['GDP ($ per capita)']
y = df_no_outliers[features]
print(X)
print(y)

import pandas as pd

# Calculate the correlation matrix
correlation_matrix = df_no_outliers.corr()

# Display the correlation matrix
print(correlation_matrix)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler


# Drop rows with NaN values in the target variable ('GDP ($ per capita)')
df = df.dropna(subset=['GDP ($ per capita)'])

# Replace commas in the entire DataFrame
df = df.replace({',': ''}, regex=True)

# Separate features and target variable
target_variable = 'GDP ($ per capita)'
X = df.drop([target_variable, 'Country', 'Region'], axis=1)  # Exclude non-numeric columns
y = df[target_variable]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#training data
y=df['GDP ($ per capita)']
X=df[['Population','Area (sq. mi.)','Pop. Density (per sq. mi.)', 'Coastline (coast/area ratio)',
       'Net migration', 'Infant mortality (per 1000 births)','Climate','Birthrate','Deathrate','Agriculture']]
train_size=int(len(X)*0.70)
train_x,test_x=X[0:train_size],X[train_size:len(X)]
train_y,test_y=y[0:train_size],y[train_size:len(X)]
print('Observation:%d'%(len(train_x)))
print("Training Observation:%d"%(len(test_x)))

# Check for missing values
missing_values = df_no_outliers.isnull().sum()
print("Missing Values:\n", missing_values)

# Select features (X) and target variable (y)
X = df_no_outliers[['Population', 'Area (sq. mi.)', 'Phones (per 1000)', 'Birthrate']]
y = df_no_outliers['GDP ($ per capita)']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Assuming 'Population' as the independent variable (X) and 'GDP ($ per capita)' as the dependent variable (y)
X = df_no_outliers['Population'].values.reshape(-1, 1)
y = df_no_outliers['GDP ($ per capita)'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Plot the regression line
plt.scatter(X_test, y_test, color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=3)
plt.xlabel('Population')
plt.ylabel('GDP ($ per capita)')
plt.title('Linear Regression: Population vs GDP')
plt.show()

import numpy as np

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt



# Drop rows with missing or NaN values
df_cleaned = df.dropna()

# Remove outliers using Z-score (assuming 'GDP ($ per capita)' as the target variable)
z_scores = (df_cleaned['GDP ($ per capita)'] - df_cleaned['GDP ($ per capita)'].mean()) / df_cleaned['GDP ($ per capita)'].std()
df_no_outliers = df_cleaned[(z_scores.abs() < 3)]

# Assuming 'Population' as the independent variable (X) and 'GDP ($ per capita)' as the dependent variable (y)
X = df_no_outliers['Population'].values.reshape(-1, 1)
y = df_no_outliers['GDP ($ per capita)'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')

# Plot the scatter plot without outliers
plt.scatter(X_test, y_test, color='black', label='Actual')
plt.scatter(X_test, y_pred, color='blue', label='Predicted')
plt.xlabel('Population')
plt.ylabel('GDP ($ per capita)')
plt.title('Linear Regression: Population vs GDP without Outliers')
plt.legend()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a box plot for GDP ($ per capita)
plt.figure(figsize=(10, 6))
sns.boxplot(x='GDP ($ per capita)', data=df, color='skyblue')
plt.title('Box Plot of GDP ($ per capita) in gdpworld')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Drop rows with missing values in the target variable
df_cleaned = df.dropna(subset=['GDP ($ per capita)'])

# Assuming 'Population' as the independent variable (X) and 'GDP ($ per capita)' as the dependent variable (y)
X = df_cleaned['Population'].values.reshape(-1, 1)
y = df_cleaned['GDP ($ per capita)'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the decision tree regressor model
model = DecisionTreeRegressor(random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')

# Plot the regression line
plt.scatter(X_test, y_test, color='black', label='Actual')
plt.scatter(X_test, y_pred, color='blue', label='Predicted')
plt.xlabel('Population')
plt.ylabel('GDP ($ per capita)')
plt.title('Decision Tree Regressor: Population vs GDP')
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Data Preprocessing
# 1.1 Handling Missing Values
df_cleaned = df.dropna(subset=['GDP ($ per capita)'])

# 1.2 Feature Engineering
X = df_cleaned['Population'].values.reshape(-1, 1)
y = df_cleaned['GDP ($ per capita)'].values

# 1.3 Model Selection
# Using Linear Regression as an example
model = LinearRegression()

# 1.4 Training the Model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model.fit(X_train, y_train)

# 2. Model Tuning
# Not performed in this simple example, but you can use GridSearchCV for hyperparameter tuning

# 3. Model Evaluation
# 3.1 Testing the Model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')

# 4. Final Presentation
# 4.1 Presentation
# Visualizing the predictions against actual values
plt.scatter(X_test, y_test, color='black', label='Actual')
plt.scatter(X_test, y_pred, color='blue', label='Predicted')
plt.xlabel('Population')
plt.ylabel('GDP ($ per capita)')
plt.title('Linear Regression: Population vs GDP')
plt.legend()
plt.show()

# 4.2 QA and Demonstration
# i) Final Presentation and QA
# - Discuss results, limitations, and answer questions
# ii) Demonstration of the Working Demo
# - Showcase how the model makes predictions on new data

selected_features = ['Country', 'Region', 'Population', 'Area (sq. mi.)',
       'Pop. Density (per sq. mi.)', 'Coastline (coast/area ratio)',
       'Net migration', 'Infant mortality (per 1000 births)',
       'GDP ($ per capita)', 'Literacy (%)', 'Phones (per 1000)', 'Arable (%)',
       'Crops (%)', 'Other (%)', 'Climate', 'Birthrate', 'Deathrate',
       'Agriculture', 'Industry', 'Service']
df_selected = df_no_outliers[selected_features]
print(df_selected )

# Split the dataset into features (X) and target variable (y)
X = df_selected.drop('GDP ($ per capita)', axis=1)
y = df_selected['GDP ($ per capita)']
print(X)
print(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train)
print(y_train)

# Initialize the Random Forest Regressor
random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)

non_linear_model = RandomForestRegressor(n_estimators=100, random_state=42)

x = df_no_outliers.iloc[:, : -1]
y = df_no_outliers.iloc[:, -1:]
print(x)
print(y)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Assuming your dataset has features in X and the target variable (GDP) in y
X = df_no_outliers.drop('GDP ($ per capita)', axis=1)
y = df_no_outliers['GDP ($ per capita)']

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the data types of your features
#print(X.dtypes)

# Use one-hot encoding for categorical variables
X_encoded = pd.get_dummies(X, drop_first=True)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Feature importance
feature_importance = rf_model.feature_importances_
sorted_idx = feature_importance.argsort()[::-1]
print(feature_importance)

#import matplotlib.pyplot as plt

# Assuming feature_importance and sorted_idx are 1D arrays
plt.bar(range(X_encoded.shape[1])[::-1], feature_importance[sorted_idx][::-1], align="center")
plt.xticks(range(X_encoded.shape[1])[::-1], X_encoded.columns[sorted_idx][::-1],rotation=90)
plt.xlabel('Feature')
plt.ylabel('Importance')

print(sorted_idx.shape)

print(feature_importance.shape)

print(X.shape)

# Step 5: Model Tuning
# Hyperparameter tuning for Random Forest
param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}
grid_search = GridSearchCV(random_forest_model, param_grid, cv=5)
grid_search.fit(X_train, y_train)
best_random_forest_model = grid_search.best_estimator_

print(sorted_idx)

print(X.shape)

print(X_train)